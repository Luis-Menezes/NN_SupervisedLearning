{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b7841a",
   "metadata": {},
   "source": [
    "# Redes Neurais - Projeto 2 \n",
    "# Aprendizado não supervisionado\n",
    "## Luis Filipe Menezes \n",
    "## RA: 164924\n",
    "\n",
    "Este caderno consiste na segunda entrega da disciplina de Redes Neurais realizada no programa de Pós Graduação em Ciência da Computação durante meu mestrado. \n",
    "\n",
    "O projeto consiste em:\n",
    "\n",
    "- Selecionar pelo menos dois datasets:\n",
    "\n",
    "    - Aplicar um modelo neural não supervisionado\n",
    "\n",
    "    - Avaliar os padrões detectados em cada conjunto:\n",
    "\n",
    "    - Clusters / outliers, etc.\n",
    "\n",
    "    - Avaliar a homogeneidade dos agrupamentos\n",
    "\n",
    "    - Variar os parâmetros do modelo (grid, taxas, número de\n",
    "neurônios, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2848daad",
   "metadata": {},
   "source": [
    "Vamos utilizar o framework minisom para gerar e treinar as redes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383e7e70",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minisom in /home/luis-menezes/miniconda3/envs/env_3.12/lib/python3.12/site-packages (2.3.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install minisom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca508dc8",
   "metadata": {},
   "source": [
    "## Mnist\n",
    "\n",
    "Vamos utilizar um subset do MNIST para acelerar o treinamento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dde298b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml, load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score, homogeneity_score\n",
    "from minisom import MiniSom\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "988f6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "SUBSET_RANGE = 0.12 # Percentual de cada dígito a ser selecionado do MNIST\n",
    "\n",
    "def prepare_mnist_data(n_samples=5000):\n",
    "    \"\"\"Prepara subset do MNIST\"\"\"\n",
    "    print(\"Carregando MNIST...\")\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    X, y = mnist.data.values, mnist.target.values.astype(int)\n",
    "    \n",
    "    # Selecionar subset estratificado\n",
    "    indices = []\n",
    "    for digit in range(10):\n",
    "        digit_indices = np.where(y == digit)[0]\n",
    "        selected = np.random.choice(digit_indices, int(n_samples * SUBSET_RANGE), replace=False)\n",
    "        indices.extend(selected)\n",
    "    \n",
    "    # Pegando um subset do mnist para acelerar o treinamento\n",
    "    # Podemos aumentar o número de amostras se necessário\n",
    "    X_subset = X[indices]\n",
    "    y_subset = y[indices]\n",
    "    \n",
    "    # Normalizar\n",
    "    scaler = StandardScaler()\n",
    "    X_normalized = scaler.fit_transform(X_subset)\n",
    "    \n",
    "    print(f\"MNIST: {X_normalized.shape[0]} amostras, {X_normalized.shape[1]} features\")\n",
    "    print(f\"Distribuição de classes: {Counter(y_subset)}\")\n",
    "    \n",
    "    X_trained, X_test, y_trained, y_test = train_test_split(X_normalized, y_subset, test_size=0.2, stratify=y_subset, random_state=42)\n",
    "\n",
    "    return X_trained, X_test, y_trained, y_test, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5803b1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando MNIST...\n",
      "MNIST: 6000 amostras, 784 features\n",
      "Distribuição de classes: Counter({np.int64(0): 600, np.int64(1): 600, np.int64(2): 600, np.int64(3): 600, np.int64(4): 600, np.int64(5): 600, np.int64(6): 600, np.int64(7): 600, np.int64(8): 600, np.int64(9): 600})\n"
     ]
    }
   ],
   "source": [
    "# Preparar os dados\n",
    "X_train, X_test, y_trained, y_test, scaler = prepare_mnist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096ff83",
   "metadata": {},
   "source": [
    "## Setup do experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d8d1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_parameters = {\n",
    "    \"map_sizes\": [7, 10, 15, 20],  # Tamanhos de mapa para variar\n",
    "    \"sigmas\": [1.0, 5.0, 10.0],\n",
    "    \"l_rates\": [0.1, 0.5, 0.9],\n",
    "    \"n_iterations\": 10000,\n",
    "    \"batch_size\": 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2233f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "som = MiniSom(x=experiment_parameters[\"map_sizes\"][0], y=experiment_parameters[\"map_sizes\"][0],\n",
    "              input_len=X_train.shape[1], # número de features\n",
    "              sigma=experiment_parameters[\"sigmas\"][0],\n",
    "              learning_rate=0.5,\n",
    "              random_seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f419f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando o SOM...\n"
     ]
    }
   ],
   "source": [
    "som.random_weights_init(X_train)\n",
    "print(\"Treinando o SOM...\")\n",
    "som.train_random(X_train, num_iteration=100000)\n",
    "\n",
    "coordinates = np.array([som.winner(x) for x in X_train]).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "528cee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando SOM com tamanho 7x7, sigma=1.0, learning_rate=0.1\n",
      "ARI: 0.2069, Silhouette Score: 0.0068, Homogeneity: 0.6496\n",
      "\n",
      "Treinando SOM com tamanho 7x7, sigma=1.0, learning_rate=0.5\n",
      "ARI: 0.2182, Silhouette Score: 0.0388, Homogeneity: 0.4973\n",
      "\n",
      "Treinando SOM com tamanho 7x7, sigma=1.0, learning_rate=0.9\n",
      "ARI: 0.2142, Silhouette Score: 0.0373, Homogeneity: 0.4354\n",
      "\n",
      "Treinando SOM com tamanho 7x7, sigma=5.0, learning_rate=0.1\n",
      "ARI: 0.2433, Silhouette Score: -0.0516, Homogeneity: 0.4989\n",
      "\n",
      "Treinando SOM com tamanho 7x7, sigma=5.0, learning_rate=0.5\n",
      "ARI: 0.1331, Silhouette Score: -0.0634, Homogeneity: 0.4283\n",
      "\n",
      "Treinando SOM com tamanho 7x7, sigma=5.0, learning_rate=0.9\n",
      "ARI: 0.1325, Silhouette Score: -0.0440, Homogeneity: 0.4181\n",
      "\n",
      "Treinando SOM com tamanho 7x7, sigma=10.0, learning_rate=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis-menezes/miniconda3/envs/env_3.12/lib/python3.12/site-packages/minisom.py:164: UserWarning: Warning: sigma might be too high for the dimension of the map.\n",
      "  warn('Warning: sigma might be too high ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.2211, Silhouette Score: -0.0434, Homogeneity: 0.3381\n",
      "\n",
      "Treinando SOM com tamanho 7x7, sigma=10.0, learning_rate=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis-menezes/miniconda3/envs/env_3.12/lib/python3.12/site-packages/minisom.py:164: UserWarning: Warning: sigma might be too high for the dimension of the map.\n",
      "  warn('Warning: sigma might be too high ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.1558, Silhouette Score: -0.0505, Homogeneity: 0.3212\n",
      "\n",
      "Treinando SOM com tamanho 7x7, sigma=10.0, learning_rate=0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis-menezes/miniconda3/envs/env_3.12/lib/python3.12/site-packages/minisom.py:164: UserWarning: Warning: sigma might be too high for the dimension of the map.\n",
      "  warn('Warning: sigma might be too high ' +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI: 0.0635, Silhouette Score: -0.0581, Homogeneity: 0.2332\n",
      "\n",
      "Treinando SOM com tamanho 10x10, sigma=1.0, learning_rate=0.1\n",
      "ARI: 0.1380, Silhouette Score: 0.0079, Homogeneity: 0.7273\n",
      "\n",
      "Treinando SOM com tamanho 10x10, sigma=1.0, learning_rate=0.5\n",
      "ARI: 0.1574, Silhouette Score: 0.0293, Homogeneity: 0.6616\n",
      "\n",
      "Treinando SOM com tamanho 10x10, sigma=1.0, learning_rate=0.9\n",
      "ARI: 0.1920, Silhouette Score: 0.0367, Homogeneity: 0.6029\n",
      "\n",
      "Treinando SOM com tamanho 10x10, sigma=5.0, learning_rate=0.1\n",
      "ARI: 0.1564, Silhouette Score: -0.0661, Homogeneity: 0.5827\n",
      "\n",
      "Treinando SOM com tamanho 10x10, sigma=5.0, learning_rate=0.5\n",
      "ARI: 0.1305, Silhouette Score: -0.0557, Homogeneity: 0.5438\n",
      "\n",
      "Treinando SOM com tamanho 10x10, sigma=5.0, learning_rate=0.9\n",
      "ARI: 0.1476, Silhouette Score: -0.0722, Homogeneity: 0.4893\n",
      "\n",
      "Treinando SOM com tamanho 10x10, sigma=10.0, learning_rate=0.1\n",
      "ARI: 0.2677, Silhouette Score: -0.0961, Homogeneity: 0.4982\n",
      "\n",
      "Treinando SOM com tamanho 10x10, sigma=10.0, learning_rate=0.5\n",
      "ARI: 0.1647, Silhouette Score: -0.1089, Homogeneity: 0.4068\n",
      "\n",
      "Treinando SOM com tamanho 10x10, sigma=10.0, learning_rate=0.9\n",
      "ARI: 0.1353, Silhouette Score: -0.1208, Homogeneity: 0.3386\n",
      "\n",
      "Treinando SOM com tamanho 15x15, sigma=1.0, learning_rate=0.1\n",
      "ARI: 0.0835, Silhouette Score: 0.0050, Homogeneity: 0.7701\n",
      "\n",
      "Treinando SOM com tamanho 15x15, sigma=1.0, learning_rate=0.5\n",
      "ARI: 0.0824, Silhouette Score: 0.0344, Homogeneity: 0.7742\n",
      "\n",
      "Treinando SOM com tamanho 15x15, sigma=1.0, learning_rate=0.9\n",
      "ARI: 0.1204, Silhouette Score: 0.0426, Homogeneity: 0.7455\n",
      "\n",
      "Treinando SOM com tamanho 15x15, sigma=5.0, learning_rate=0.1\n",
      "ARI: 0.0737, Silhouette Score: -0.0629, Homogeneity: 0.6833\n",
      "\n",
      "Treinando SOM com tamanho 15x15, sigma=5.0, learning_rate=0.5\n",
      "ARI: 0.0740, Silhouette Score: -0.0693, Homogeneity: 0.6632\n",
      "\n",
      "Treinando SOM com tamanho 15x15, sigma=5.0, learning_rate=0.9\n",
      "ARI: 0.0761, Silhouette Score: -0.0750, Homogeneity: 0.6287\n",
      "\n",
      "Treinando SOM com tamanho 15x15, sigma=10.0, learning_rate=0.1\n",
      "ARI: 0.1490, Silhouette Score: -0.1163, Homogeneity: 0.5736\n",
      "\n",
      "Treinando SOM com tamanho 15x15, sigma=10.0, learning_rate=0.5\n",
      "ARI: 0.1470, Silhouette Score: -0.1062, Homogeneity: 0.5209\n",
      "\n",
      "Treinando SOM com tamanho 15x15, sigma=10.0, learning_rate=0.9\n",
      "ARI: 0.1196, Silhouette Score: -0.1146, Homogeneity: 0.4795\n",
      "\n",
      "Treinando SOM com tamanho 20x20, sigma=1.0, learning_rate=0.1\n",
      "ARI: 0.0667, Silhouette Score: -0.0050, Homogeneity: 0.7959\n",
      "\n",
      "Treinando SOM com tamanho 20x20, sigma=1.0, learning_rate=0.5\n",
      "ARI: 0.0516, Silhouette Score: 0.0399, Homogeneity: 0.8256\n",
      "\n",
      "Treinando SOM com tamanho 20x20, sigma=1.0, learning_rate=0.9\n",
      "ARI: 0.0580, Silhouette Score: 0.0404, Homogeneity: 0.8096\n",
      "\n",
      "Treinando SOM com tamanho 20x20, sigma=5.0, learning_rate=0.1\n",
      "ARI: 0.0506, Silhouette Score: -0.0741, Homogeneity: 0.7551\n",
      "\n",
      "Treinando SOM com tamanho 20x20, sigma=5.0, learning_rate=0.5\n",
      "ARI: 0.0568, Silhouette Score: -0.0605, Homogeneity: 0.7366\n",
      "\n",
      "Treinando SOM com tamanho 20x20, sigma=5.0, learning_rate=0.9\n",
      "ARI: 0.0667, Silhouette Score: -0.0617, Homogeneity: 0.7277\n",
      "\n",
      "Treinando SOM com tamanho 20x20, sigma=10.0, learning_rate=0.1\n",
      "ARI: 0.1057, Silhouette Score: -0.1225, Homogeneity: 0.6710\n",
      "\n",
      "Treinando SOM com tamanho 20x20, sigma=10.0, learning_rate=0.5\n",
      "ARI: 0.1672, Silhouette Score: -0.1011, Homogeneity: 0.6176\n",
      "\n",
      "Treinando SOM com tamanho 20x20, sigma=10.0, learning_rate=0.9\n",
      "ARI: 0.1105, Silhouette Score: -0.1231, Homogeneity: 0.6012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "experiment_results = []\n",
    "\n",
    "for map_size in experiment_parameters[\"map_sizes\"]:\n",
    "    for sigma in experiment_parameters[\"sigmas\"]:\n",
    "        for l_rate in experiment_parameters[\"l_rates\"]:\n",
    "            print(f\"Treinando SOM com tamanho {map_size}x{map_size}, sigma={sigma}, learning_rate={l_rate}\")\n",
    "            som = MiniSom(x=map_size, y=map_size,\n",
    "                          input_len=X_train.shape[1],\n",
    "                          sigma=sigma,\n",
    "                          learning_rate=l_rate,\n",
    "                          random_seed=42)\n",
    "            som.random_weights_init(X_train)\n",
    "            som.train_random(X_train, num_iteration=experiment_parameters[\"n_iterations\"])\n",
    "            \n",
    "            # Avaliar o desempenho\n",
    "            coordinates = np.array([som.winner(x) for x in X_train]).astype(float)\n",
    "            ari = adjusted_rand_score(y_trained, [c[0]*map_size + c[1] for c in coordinates])\n",
    "            sil_score = silhouette_score(X_train, [c[0]*map_size + c[1] for c in coordinates])\n",
    "            homogeneity = homogeneity_score(y_trained, [c[0]*map_size + c[1] for c in coordinates])\n",
    "            results = {\n",
    "                \"map_size\": map_size,\n",
    "                \"sigma\": sigma,\n",
    "                \"learning_rate\": l_rate,\n",
    "                \"ARI\": ari,\n",
    "                \"Silhouette Score\": sil_score,\n",
    "                \"Homogeneity\": homogeneity\n",
    "            }\n",
    "            experiment_results.append(results)\n",
    "            print(f\"ARI: {ari:.4f}, Silhouette Score: {sil_score:.4f}, Homogeneity: {homogeneity:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4fc913",
   "metadata": {},
   "source": [
    "## Implementing SOM using tensor flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac430c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "class SOMGPU:\n",
    "    def __init__(self, m, n, dim, learning_rate=0.1, sigma=1.0):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.dim = dim\n",
    "        self.learning_rate_initial = learning_rate\n",
    "        self.sigma_initial = sigma\n",
    "        \n",
    "        # Otimização: Colocar tudo na GPU\n",
    "        with tf.device('/GPU:0'):\n",
    "            self.weights = tf.Variable(\n",
    "                tf.random.normal([m * n, dim]), \n",
    "                trainable=True,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            self.locations = tf.constant(\n",
    "                [[i, j] for i in range(m) for j in range(n)], \n",
    "                dtype=tf.float32\n",
    "            )\n",
    "    \n",
    "    # O @tf.function é mais eficiente em operações maiores\n",
    "    # @tf.function\n",
    "    def update_weights_batch(self, batch_data, iteration, max_iterations):\n",
    "        # 1. Encontrar o BMU para CADA amostra no lote\n",
    "        # Forma do batch_data: (batch_size, dim)\n",
    "        # Forma dos weights:   (m*n, dim)\n",
    "        # Usamos broadcasting para calcular todas as distâncias de uma vez\n",
    "        expanded_weights = tf.expand_dims(self.weights, axis=0) # (1, m*n, dim)\n",
    "        expanded_batch = tf.expand_dims(batch_data, axis=1) # (batch_size, 1, dim)\n",
    "        \n",
    "        # Distâncias para cada amostra do lote para cada neurônio\n",
    "        distances_sq = tf.reduce_sum(tf.square(expanded_weights - expanded_batch), axis=2) # (batch_size, m*n)\n",
    "        bmu_indices = tf.argmin(distances_sq, axis=1) # (batch_size,)\n",
    "\n",
    "        # 2. Atualizar pesos para CADA amostra no lote\n",
    "        # Decaimento baseado na iteração global, não na época\n",
    "        current_sigma = self.sigma_initial * tf.exp(-tf.cast(iteration, tf.float32) / max_iterations)\n",
    "        current_lr = self.learning_rate_initial * tf.exp(-tf.cast(iteration, tf.float32) / max_iterations)\n",
    "        \n",
    "        bmu_locations = tf.gather(self.locations, bmu_indices) # (batch_size, 2)\n",
    "        \n",
    "        # Distância de todos os neurônios para cada BMU do lote\n",
    "        # (1, m*n, 2) - (batch_size, 1, 2) -> (batch_size, m*n, 2)\n",
    "        dist_to_bmu_sq = tf.reduce_sum(\n",
    "            tf.square(tf.expand_dims(self.locations, 0) - tf.expand_dims(bmu_locations, 1)), axis=2\n",
    "        )\n",
    "        \n",
    "        # Função de vizinhança para cada amostra do lote\n",
    "        neighborhood = tf.exp(-dist_to_bmu_sq / (2 * tf.square(current_sigma))) # (batch_size, m*n)\n",
    "        \n",
    "        # Cálculo do delta (mudança nos pesos)\n",
    "        # (batch_size, m*n, 1) * ((batch_size, 1, dim) - (1, m*n, dim))\n",
    "        delta_numerator = tf.expand_dims(neighborhood, axis=2) * (expanded_batch - expanded_weights)\n",
    "        \n",
    "        # Média dos deltas em todo o lote\n",
    "        delta = tf.reduce_mean(delta_numerator, axis=0) * current_lr\n",
    "        \n",
    "        # Aplica a atualização\n",
    "        self.weights.assign_add(delta)\n",
    "\n",
    "    def train(self, data, epochs, batch_size=32):\n",
    "        # Usar tf.data.Dataset é muito mais eficiente para lidar com dados\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(data.astype(np.float32))\n",
    "        dataset = dataset.shuffle(buffer_size=len(data)).batch(batch_size)\n",
    "        \n",
    "        max_iterations = epochs * len(list(dataset))\n",
    "        \n",
    "        for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "            for i, batch in enumerate(dataset):\n",
    "                iteration = epoch * len(list(dataset)) + i\n",
    "                self.update_weights_batch(batch, iteration, max_iterations)\n",
    "\n",
    "    def map_vects(self, data):\n",
    "        \"\"\" Retorna as coordenadas 2D para cada vetor de entrada \"\"\"\n",
    "        data_tf = tf.constant(data, dtype=tf.float32)\n",
    "        \n",
    "        expanded_weights = tf.expand_dims(self.weights, axis=0)\n",
    "        expanded_data = tf.expand_dims(data_tf, axis=1)\n",
    "        \n",
    "        distances_sq = tf.reduce_sum(tf.square(expanded_weights - expanded_data), axis=2)\n",
    "        bmu_indices = tf.argmin(distances_sq, axis=1)\n",
    "        \n",
    "        coords = tf.gather(self.locations, bmu_indices)\n",
    "        return coords.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de41ac8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a383881e6c764be89d85f1be5495e980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 17:27:23.420293: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "som_gpu = SOMGPU(m=7, n=7, dim=X_mnist.shape[1], learning_rate=0.5, sigma=5.0)\n",
    "som_gpu.train(X_mnist, epochs=10, batch_size=32)  # Treinando o SOM na GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be251bb8",
   "metadata": {},
   "source": [
    "# Redes Neurais - Projeto 4\n",
    "# Modelos Recorrentes\n",
    "---------------------\n",
    "### Luis Filipe Menezes\n",
    "#### RA: 164924\n",
    "\n",
    "## 1. Objetivos:\n",
    "Este caderno consiste na terceira entrega da disciplina de Redes Neurais realizada no programa de Pós Graduação em Ciência da Computação durante meu mestrado.\n",
    "\n",
    "O projeto tem como objetivo:\n",
    "\n",
    "- Implementar um modelo LSTM ou GRU para uma das tarefas abaixo:\n",
    "1. Classificação de série temporais. O modelo deve receber uma janela temporal (qualquer tipo de dado) e classificar o conteúdo da janela.\n",
    "2. Previsão. Treinar um modelo para predizer o valor de uma variável no instante t+k. O modelo deve receber os dados da série temporal (instantes anteriores a t – verificar tamanho da janela) e predizer um favor futuro. k a distância da predição. Por exemplo, podemos alimentar um modelo com dados de uma dada empresa (i.e. PETR3) e tentar predizer qual será o valor da ação daqui 5 dias (k==5)\n",
    "3. Autoencoder recorrente. O modelo deve mapear a série temporal na própria série. O objetivo será avaliar como os dados estão representados no espaço latente\n",
    "\n",
    "Para este projeto foi escolhido a tarefa 2 de previsão de ocupação de ambientes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4397c6",
   "metadata": {},
   "source": [
    "# 1. Aquisição dos dados\n",
    "Utilizaremos o banco de dados [Room Occupancy Estimation](https://archive.ics.uci.edu/dataset/864/room+occupancy+estimation) that is a dataset with a precise number of occupants in a room using multiple non-intrusive environmental sensors like temperature, light, sound, CO2 and PIR.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21931f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 864, 'name': 'Room Occupancy Estimation', 'repository_url': 'https://archive.ics.uci.edu/dataset/864/room+occupancy+estimation', 'data_url': 'https://archive.ics.uci.edu/static/public/864/data.csv', 'abstract': 'Data set for estimating the precise number of occupants in a room using multiple non-intrusive environmental sensors like temperature, light, sound, CO2 and PIR.', 'area': 'Computer Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate', 'Time-Series'], 'num_instances': 10129, 'num_features': 18, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['Room_Occupancy_Count'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2018, 'last_updated': 'Wed Aug 16 2023', 'dataset_doi': '10.24432/C5P605', 'creators': ['Adarsh Pal Singh', 'Sachin Chaudhari'], 'intro_paper': {'ID': 275, 'type': 'NATIVE', 'title': 'Machine Learning-Based Occupancy Estimation Using Multivariate Sensor Nodes', 'authors': 'A. Singh, Vivek Jain, S. Chaudhari, F. Kraemer, S. Werner, V. Garg', 'venue': '2018 IEEE Globecom Workshops (GC Wkshps)', 'year': 2018, 'journal': None, 'DOI': None, 'URL': 'https://www.semanticscholar.org/paper/e631ea26f0fd88541f42b4e049d63d6b52d6d3ac', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'The experimental testbed for occupancy estimation was deployed in a 6m x 4.6m room. The setup consisted of 7 sensor nodes and one edge node in a star configuration with the sensor nodes transmitting data to the edge every 30s using wireless transceivers. No HVAC systems were in use while the dataset was being collected.\\n\\nFive different types of non-intrusive sensors were used in this experiment: temperature, light, sound, CO2 and digital passive infrared (PIR). The CO2, sound and PIR sensors needed manual calibration. For the CO2 sensor, zero-point calibration was manually done before its first use by keeping it in a clean environment for over 20 minutes and then pulling the calibration pin (HD pin) low for over 7s. The sound sensor is essentially a microphone with a variable-gain analog amplifier attached to it. Therefore, the output of this sensor is analog which is read by the microcontrollerâ€™s ADC in volts. The potentiometer tied to the gain of the amplifier was adjusted to ensure the highest sensitivity. The PIR sensor has two trimpots: one to tweak the sensitivity and the other to tweak the time for which the output stays high after detecting motion. Both of these were adjusted to the highest values. Sensor nodes S1-S4 consisted of temperature, light and sound sensors, S5 had a CO2 sensor and S6 and S7 had one PIR sensor each that were deployed on the ceiling ledges at an angle that maximized the sensorâ€™s field of view for motion detection.\\n\\nThe data was collected for a period of 4 days in a controlled manner with the occupancy in the room varying between 0 and 3 people. The ground truth of the occupancy count in the room was noted manually.\\n\\nPlease refer to our publications for more details.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Date: YYYY/MM/DD\\nTime: HH:MM:SS\\nTemperature: In degree Celsius\\nLight: In Lux\\nSound: In Volts (amplifier output read by ADC)\\nCO2: In PPM\\nCO2 Slope: Slope of CO2 values taken in a sliding window\\nPIR: Binary value conveying motion detection\\nRoom_Occupancy_Count: Ground Truth', 'citation': 'If you use this dataset in your research, please cite the following paper:\\nAdarsh Pal Singh, Vivek Jain, Sachin Chaudhari, Frank Alexander Kraemer, Stefan Werner and Vishal Garg, \"Machine Learning-Based Occupancy Estimation Using Multivariate Sensor Nodes,\" in 2018 IEEE Globecom Workshops (GC Wkshps), 2018.'}}\n",
      "                    name     role        type demographic  \\\n",
      "0                   Date  Feature        Date        None   \n",
      "1                   Time  Feature        Date        None   \n",
      "2                S1_Temp  Feature  Continuous        None   \n",
      "3                S2_Temp  Feature  Continuous        None   \n",
      "4                S3_Temp  Feature  Continuous        None   \n",
      "5                S4_Temp  Feature  Continuous        None   \n",
      "6               S1_Light  Feature     Integer        None   \n",
      "7               S2_Light  Feature     Integer        None   \n",
      "8               S3_Light  Feature     Integer        None   \n",
      "9               S4_Light  Feature     Integer        None   \n",
      "10              S1_Sound  Feature  Continuous        None   \n",
      "11              S2_Sound  Feature  Continuous        None   \n",
      "12              S3_Sound  Feature  Continuous        None   \n",
      "13              S4_Sound  Feature  Continuous        None   \n",
      "14                S5_CO2  Feature     Integer        None   \n",
      "15          S5_CO2_Slope  Feature  Continuous        None   \n",
      "16                S6_PIR  Feature      Binary        None   \n",
      "17                S7_PIR  Feature     Integer        None   \n",
      "18  Room_Occupancy_Count   Target     Integer        None   \n",
      "\n",
      "                                      description       units missing_values  \n",
      "0                                            None  YYYY/MM/DD             no  \n",
      "1                                            None    HH:MM:SS             no  \n",
      "2                                            None           C             no  \n",
      "3                                            None           C             no  \n",
      "4                                            None           C             no  \n",
      "5                                            None           C             no  \n",
      "6                                            None         Lux             no  \n",
      "7                                            None         Lux             no  \n",
      "8                                            None         Lux             no  \n",
      "9                                            None         Lux             no  \n",
      "10                   amplifier output read by ADC       Volts             no  \n",
      "11                   amplifier output read by ADC       Volts             no  \n",
      "12                   amplifier output read by ADC       Volts             no  \n",
      "13                   amplifier output read by ADC       Volts             no  \n",
      "14                                           None         PPM             no  \n",
      "15  Slope of CO2 values taken in a sliding window        None             no  \n",
      "16        Binary value conveying motion detection        None             no  \n",
      "17        Binary value conveying motion detection        None             no  \n",
      "18                                   Ground Truth        None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "room_occupancy_estimation = fetch_ucirepo(id=864) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = room_occupancy_estimation.data.features \n",
    "y = room_occupancy_estimation.data.targets \n",
    "  \n",
    "# metadata \n",
    "# print(room_occupancy_estimation.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(room_occupancy_estimation.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6564ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x71d142d195d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(24)\n",
    "torch.manual_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2a946c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (10129, 18)\n",
      "\n",
      "Feature columns:\n",
      "['Date', 'Time', 'S1_Temp', 'S2_Temp', 'S3_Temp', 'S4_Temp', 'S1_Light', 'S2_Light', 'S3_Light', 'S4_Light', 'S1_Sound', 'S2_Sound', 'S3_Sound', 'S4_Sound', 'S5_CO2', 'S5_CO2_Slope', 'S6_PIR', 'S7_PIR']\n",
      "\n",
      "Target shape: (10129, 1)\n",
      "\n",
      "Target column:\n",
      "['Room_Occupancy_Count']\n",
      "\n",
      "Dataset info:\n",
      "Features: 18\n",
      "Samples: 10129\n",
      "Missing values in X: 0\n",
      "Missing values in y: 0\n",
      "\n",
      "First 5 rows of features:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "S1_Temp",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S2_Temp",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S3_Temp",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S4_Temp",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S1_Light",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "S2_Light",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "S3_Light",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "S4_Light",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "S1_Sound",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S2_Sound",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S3_Sound",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S4_Sound",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S5_CO2",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "S5_CO2_Slope",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "S6_PIR",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "S7_PIR",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e5629829-f4d0-43a2-a0c5-652571762158",
       "rows": [
        [
         "0",
         "2017/12/22",
         "10:49:41",
         "24.94",
         "24.75",
         "24.56",
         "25.38",
         "121",
         "34",
         "53",
         "40",
         "0.08",
         "0.19",
         "0.06",
         "0.06",
         "390",
         "0.769230769231",
         "0",
         "0"
        ],
        [
         "1",
         "2017/12/22",
         "10:50:12",
         "24.94",
         "24.75",
         "24.56",
         "25.44",
         "121",
         "33",
         "53",
         "40",
         "0.93",
         "0.05",
         "0.06",
         "0.06",
         "390",
         "0.646153846154",
         "0",
         "0"
        ],
        [
         "2",
         "2017/12/22",
         "10:50:42",
         "25.0",
         "24.75",
         "24.5",
         "25.44",
         "121",
         "34",
         "53",
         "40",
         "0.43",
         "0.11",
         "0.08",
         "0.06",
         "390",
         "0.519230769231",
         "0",
         "0"
        ],
        [
         "3",
         "2017/12/22",
         "10:51:13",
         "25.0",
         "24.75",
         "24.56",
         "25.44",
         "121",
         "34",
         "53",
         "40",
         "0.41",
         "0.1",
         "0.1",
         "0.09",
         "390",
         "0.388461538462",
         "0",
         "0"
        ],
        [
         "4",
         "2017/12/22",
         "10:51:44",
         "25.0",
         "24.75",
         "24.56",
         "25.44",
         "121",
         "34",
         "54",
         "40",
         "0.18",
         "0.06",
         "0.06",
         "0.06",
         "390",
         "0.253846153846",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>S1_Temp</th>\n",
       "      <th>S2_Temp</th>\n",
       "      <th>S3_Temp</th>\n",
       "      <th>S4_Temp</th>\n",
       "      <th>S1_Light</th>\n",
       "      <th>S2_Light</th>\n",
       "      <th>S3_Light</th>\n",
       "      <th>S4_Light</th>\n",
       "      <th>S1_Sound</th>\n",
       "      <th>S2_Sound</th>\n",
       "      <th>S3_Sound</th>\n",
       "      <th>S4_Sound</th>\n",
       "      <th>S5_CO2</th>\n",
       "      <th>S5_CO2_Slope</th>\n",
       "      <th>S6_PIR</th>\n",
       "      <th>S7_PIR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:49:41</td>\n",
       "      <td>24.94</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.38</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:50:12</td>\n",
       "      <td>24.94</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.44</td>\n",
       "      <td>121</td>\n",
       "      <td>33</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:50:42</td>\n",
       "      <td>25.00</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.50</td>\n",
       "      <td>25.44</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:51:13</td>\n",
       "      <td>25.00</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.44</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>390</td>\n",
       "      <td>0.388462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017/12/22</td>\n",
       "      <td>10:51:44</td>\n",
       "      <td>25.00</td>\n",
       "      <td>24.75</td>\n",
       "      <td>24.56</td>\n",
       "      <td>25.44</td>\n",
       "      <td>121</td>\n",
       "      <td>34</td>\n",
       "      <td>54</td>\n",
       "      <td>40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>390</td>\n",
       "      <td>0.253846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Time  S1_Temp  S2_Temp  S3_Temp  S4_Temp  S1_Light  \\\n",
       "0  2017/12/22  10:49:41    24.94    24.75    24.56    25.38       121   \n",
       "1  2017/12/22  10:50:12    24.94    24.75    24.56    25.44       121   \n",
       "2  2017/12/22  10:50:42    25.00    24.75    24.50    25.44       121   \n",
       "3  2017/12/22  10:51:13    25.00    24.75    24.56    25.44       121   \n",
       "4  2017/12/22  10:51:44    25.00    24.75    24.56    25.44       121   \n",
       "\n",
       "   S2_Light  S3_Light  S4_Light  S1_Sound  S2_Sound  S3_Sound  S4_Sound  \\\n",
       "0        34        53        40      0.08      0.19      0.06      0.06   \n",
       "1        33        53        40      0.93      0.05      0.06      0.06   \n",
       "2        34        53        40      0.43      0.11      0.08      0.06   \n",
       "3        34        53        40      0.41      0.10      0.10      0.09   \n",
       "4        34        54        40      0.18      0.06      0.06      0.06   \n",
       "\n",
       "   S5_CO2  S5_CO2_Slope  S6_PIR  S7_PIR  \n",
       "0     390      0.769231       0       0  \n",
       "1     390      0.646154       0       0  \n",
       "2     390      0.519231       0       0  \n",
       "3     390      0.388462       0       0  \n",
       "4     390      0.253846       0       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of target:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Room_Occupancy_Count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "3d721d8b-3a7b-4d03-bb65-e1ab350b14e6",
       "rows": [
        [
         "0",
         "1"
        ],
        [
         "1",
         "1"
        ],
        [
         "2",
         "1"
        ],
        [
         "3",
         "1"
        ],
        [
         "4",
         "1"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Room_Occupancy_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Room_Occupancy_Count\n",
       "0                     1\n",
       "1                     1\n",
       "2                     1\n",
       "3                     1\n",
       "4                     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"\\nFeature columns:\")\n",
    "print(X.columns.tolist())\n",
    "print(\"\\nTarget shape:\", y.shape)\n",
    "print(\"\\nTarget column:\")\n",
    "print(y.columns.tolist())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nDataset info:\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {X.shape[0]}\")\n",
    "print(f\"Missing values in X: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in y: {y.isnull().sum().sum()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of features:\")\n",
    "display(X.head())\n",
    "print(\"\\nFirst 5 rows of target:\")\n",
    "display(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4677d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced TimeSeriesPreprocessor that handles all data types\n",
    "class TimeSeriesPreprocessor:\n",
    "    \"\"\"Enhanced preprocessor for time series data with sliding windows\"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length=24, prediction_horizon=1, test_size=0.2):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.test_size = test_size\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        \n",
    "    def preprocess_features(self, X):\n",
    "        \"\"\"Preprocess features to handle different data types\"\"\"\n",
    "        print(\"Preprocessing features...\")\n",
    "        \n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        # Check for non-numeric columns\n",
    "        non_numeric_cols = X_processed.select_dtypes(include=['object']).columns\n",
    "        if len(non_numeric_cols) > 0:\n",
    "            print(f\"Found non-numeric columns: {non_numeric_cols.tolist()}\")\n",
    "            \n",
    "            for col in non_numeric_cols:\n",
    "                print(f\"Processing non-numeric column: {col}\")\n",
    "                \n",
    "                # Try to convert to datetime first\n",
    "                try:\n",
    "                    datetime_series = pd.to_datetime(X_processed[col])\n",
    "                    print(f\"  Converting {col} to datetime features\")\n",
    "                    \n",
    "                    # Extract datetime features\n",
    "                    X_processed[f'{col}_year'] = datetime_series.dt.year\n",
    "                    X_processed[f'{col}_month'] = datetime_series.dt.month\n",
    "                    X_processed[f'{col}_day'] = datetime_series.dt.day\n",
    "                    X_processed[f'{col}_hour'] = datetime_series.dt.hour\n",
    "                    X_processed[f'{col}_minute'] = datetime_series.dt.minute\n",
    "                    X_processed[f'{col}_dayofweek'] = datetime_series.dt.dayofweek\n",
    "                    \n",
    "                    # Cyclical encoding\n",
    "                    X_processed[f'{col}_hour_sin'] = np.sin(2 * np.pi * datetime_series.dt.hour / 24)\n",
    "                    X_processed[f'{col}_hour_cos'] = np.cos(2 * np.pi * datetime_series.dt.hour / 24)\n",
    "                    X_processed[f'{col}_month_sin'] = np.sin(2 * np.pi * datetime_series.dt.month / 12)\n",
    "                    X_processed[f'{col}_month_cos'] = np.cos(2 * np.pi * datetime_series.dt.month / 12)\n",
    "                    X_processed[f'{col}_dayofweek_sin'] = np.sin(2 * np.pi * datetime_series.dt.dayofweek / 7)\n",
    "                    X_processed[f'{col}_dayofweek_cos'] = np.cos(2 * np.pi * datetime_series.dt.dayofweek / 7)\n",
    "                    \n",
    "                    # Drop original column\n",
    "                    X_processed = X_processed.drop(columns=[col])\n",
    "                    \n",
    "                except ValueError:\n",
    "                    print(f\"  Could not convert {col} to datetime, trying categorical encoding\")\n",
    "                    \n",
    "                    # Try categorical encoding\n",
    "                    unique_values = X_processed[col].nunique()\n",
    "                    if unique_values < 50:  # Arbitrary threshold for categorical\n",
    "                        # Label encoding for categorical variables\n",
    "                        from sklearn.preprocessing import LabelEncoder\n",
    "                        le = LabelEncoder()\n",
    "                        X_processed[f'{col}_encoded'] = le.fit_transform(X_processed[col].astype(str))\n",
    "                        X_processed = X_processed.drop(columns=[col])\n",
    "                    else:\n",
    "                        print(f\"  Dropping {col} - too many unique values or unsupported type\")\n",
    "                        X_processed = X_processed.drop(columns=[col])\n",
    "        \n",
    "        # Ensure all remaining columns are numeric\n",
    "        X_processed = X_processed.select_dtypes(include=[np.number])\n",
    "        \n",
    "        print(f\"Final processed features: {X_processed.shape[1]} columns\")\n",
    "        print(f\"Feature names: {X_processed.columns.tolist()}\")\n",
    "        \n",
    "        return X_processed\n",
    "        \n",
    "    def create_sequences(self, features, targets):\n",
    "        \"\"\"Create sliding window sequences\"\"\"\n",
    "        X_seq, y_seq = [], []\n",
    "        \n",
    "        for i in range(len(features) - self.sequence_length - self.prediction_horizon + 1):\n",
    "            # Input sequence\n",
    "            X_seq.append(features[i:i + self.sequence_length])\n",
    "            # Target (predict t+prediction_horizon)\n",
    "            y_seq.append(targets[i + self.sequence_length + self.prediction_horizon - 1])\n",
    "            \n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"Fit scalers and transform data\"\"\"\n",
    "        print(\"Preprocessing time series data...\")\n",
    "        \n",
    "        # Preprocess features to handle different data types\n",
    "        X_processed = self.preprocess_features(X)\n",
    "        \n",
    "        # Handle different target formats\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            if y.shape[1] == 1:\n",
    "                target_values = y.iloc[:, 0].values\n",
    "            else:\n",
    "                print(f\"Warning: Multiple target columns found: {y.columns.tolist()}\")\n",
    "                print(\"Using first column as target\")\n",
    "                target_values = y.iloc[:, 0].values\n",
    "        elif isinstance(y, pd.Series):\n",
    "            target_values = y.values\n",
    "        else:\n",
    "            target_values = y\n",
    "        \n",
    "        print(f\"Target variable shape: {target_values.shape}\")\n",
    "        print(f\"Target variable range: {target_values.min():.2f} to {target_values.max():.2f}\")\n",
    "        \n",
    "        # Scale features and targets\n",
    "        X_scaled = self.feature_scaler.fit_transform(X_processed)\n",
    "        y_scaled = self.target_scaler.fit_transform(target_values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Create sequences\n",
    "        X_seq, y_seq = self.create_sequences(X_scaled, y_scaled)\n",
    "        \n",
    "        # Train-test split (temporal split)\n",
    "        split_idx = int(len(X_seq) * (1 - self.test_size))\n",
    "        \n",
    "        X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "        y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "        \n",
    "        print(f\"Training sequences: {X_train.shape}\")\n",
    "        print(f\"Test sequences: {X_test.shape}\")\n",
    "        print(f\"Sequence length: {self.sequence_length}\")\n",
    "        print(f\"Prediction horizon: {self.prediction_horizon}\")\n",
    "        \n",
    "        return (X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    def inverse_transform_target(self, y_scaled):\n",
    "        \"\"\"Convert scaled targets back to original scale\"\"\"\n",
    "        return self.target_scaler.inverse_transform(y_scaled.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "742b088e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATETIME PREPROCESSING ===\n",
      "Found datetime column: Date\n",
      "Found datetime column: Time\n",
      "Processing datetime column: Date\n",
      "Processing datetime column: Time\n",
      "Original features: 18\n",
      "Processed features: 42\n",
      "New feature columns: ['S1_Temp', 'S2_Temp', 'S3_Temp', 'S4_Temp', 'S1_Light', 'S2_Light', 'S3_Light', 'S4_Light', 'S1_Sound', 'S2_Sound', 'S3_Sound', 'S4_Sound', 'S5_CO2', 'S5_CO2_Slope', 'S6_PIR', 'S7_PIR', 'Date_year', 'Date_month', 'Date_day', 'Date_hour', 'Date_minute', 'Date_dayofweek', 'Date_dayofyear', 'Date_hour_sin', 'Date_hour_cos', 'Date_month_sin', 'Date_month_cos', 'Date_dayofweek_sin', 'Date_dayofweek_cos', 'Time_year', 'Time_month', 'Time_day', 'Time_hour', 'Time_minute', 'Time_dayofweek', 'Time_dayofyear', 'Time_hour_sin', 'Time_hour_cos', 'Time_month_sin', 'Time_month_cos', 'Time_dayofweek_sin', 'Time_dayofweek_cos']\n",
      "=== DATA DEBUGGING ===\n",
      "Features (X) shape: (10129, 42)\n",
      "Features columns: ['S1_Temp', 'S2_Temp', 'S3_Temp', 'S4_Temp', 'S1_Light', 'S2_Light', 'S3_Light', 'S4_Light', 'S1_Sound', 'S2_Sound', 'S3_Sound', 'S4_Sound', 'S5_CO2', 'S5_CO2_Slope', 'S6_PIR', 'S7_PIR', 'Date_year', 'Date_month', 'Date_day', 'Date_hour', 'Date_minute', 'Date_dayofweek', 'Date_dayofyear', 'Date_hour_sin', 'Date_hour_cos', 'Date_month_sin', 'Date_month_cos', 'Date_dayofweek_sin', 'Date_dayofweek_cos', 'Time_year', 'Time_month', 'Time_day', 'Time_hour', 'Time_minute', 'Time_dayofweek', 'Time_dayofyear', 'Time_hour_sin', 'Time_hour_cos', 'Time_month_sin', 'Time_month_cos', 'Time_dayofweek_sin', 'Time_dayofweek_cos']\n",
      "Features data types: S1_Temp               float64\n",
      "S2_Temp               float64\n",
      "S3_Temp               float64\n",
      "S4_Temp               float64\n",
      "S1_Light                int64\n",
      "S2_Light                int64\n",
      "S3_Light                int64\n",
      "S4_Light                int64\n",
      "S1_Sound              float64\n",
      "S2_Sound              float64\n",
      "S3_Sound              float64\n",
      "S4_Sound              float64\n",
      "S5_CO2                  int64\n",
      "S5_CO2_Slope          float64\n",
      "S6_PIR                  int64\n",
      "S7_PIR                  int64\n",
      "Date_year               int32\n",
      "Date_month              int32\n",
      "Date_day                int32\n",
      "Date_hour               int32\n",
      "Date_minute             int32\n",
      "Date_dayofweek          int32\n",
      "Date_dayofyear          int32\n",
      "Date_hour_sin         float64\n",
      "Date_hour_cos         float64\n",
      "Date_month_sin        float64\n",
      "Date_month_cos        float64\n",
      "Date_dayofweek_sin    float64\n",
      "Date_dayofweek_cos    float64\n",
      "Time_year               int32\n",
      "Time_month              int32\n",
      "Time_day                int32\n",
      "Time_hour               int32\n",
      "Time_minute             int32\n",
      "Time_dayofweek          int32\n",
      "Time_dayofyear          int32\n",
      "Time_hour_sin         float64\n",
      "Time_hour_cos         float64\n",
      "Time_month_sin        float64\n",
      "Time_month_cos        float64\n",
      "Time_dayofweek_sin    float64\n",
      "Time_dayofweek_cos    float64\n",
      "dtype: object\n",
      "\n",
      "Target (y) type: <class 'pandas.core.frame.DataFrame'>\n",
      "Target (y) shape: (10129, 1)\n",
      "Target columns: ['Room_Occupancy_Count']\n",
      "Target data types: Room_Occupancy_Count    int64\n",
      "dtype: object\n",
      "\n",
      "Missing values in X: 0\n",
      "Missing values in y: 0\n",
      "Using single target column: Room_Occupancy_Count\n",
      "Target series shape: (10129,)\n",
      "Target range: 0.00 to 3.00\n",
      "Target mean: 0.40\n",
      "Preprocessing time series data...\n",
      "Preprocessing features...\n",
      "Final processed features: 42 columns\n",
      "Feature names: ['S1_Temp', 'S2_Temp', 'S3_Temp', 'S4_Temp', 'S1_Light', 'S2_Light', 'S3_Light', 'S4_Light', 'S1_Sound', 'S2_Sound', 'S3_Sound', 'S4_Sound', 'S5_CO2', 'S5_CO2_Slope', 'S6_PIR', 'S7_PIR', 'Date_year', 'Date_month', 'Date_day', 'Date_hour', 'Date_minute', 'Date_dayofweek', 'Date_dayofyear', 'Date_hour_sin', 'Date_hour_cos', 'Date_month_sin', 'Date_month_cos', 'Date_dayofweek_sin', 'Date_dayofweek_cos', 'Time_year', 'Time_month', 'Time_day', 'Time_hour', 'Time_minute', 'Time_dayofweek', 'Time_dayofyear', 'Time_hour_sin', 'Time_hour_cos', 'Time_month_sin', 'Time_month_cos', 'Time_dayofweek_sin', 'Time_dayofweek_cos']\n",
      "Target variable shape: (10129,)\n",
      "Target variable range: 0.00 to 3.00\n",
      "Training sequences: (8084, 24, 42)\n",
      "Test sequences: (2021, 24, 42)\n",
      "Sequence length: 24\n",
      "Prediction horizon: 1\n"
     ]
    }
   ],
   "source": [
    "# Enhanced preprocessing to handle datetime columns\n",
    "def preprocess_datetime_features(X, y):\n",
    "    \"\"\"Preprocess datetime columns and extract temporal features\"\"\"\n",
    "    \n",
    "    print(\"=== DATETIME PREPROCESSING ===\")\n",
    "    \n",
    "    # Make a copy to avoid modifying original data\n",
    "    X_processed = X.copy()\n",
    "    \n",
    "    # Identify datetime columns\n",
    "    datetime_columns = []\n",
    "    for col in X_processed.columns:\n",
    "        if X_processed[col].dtype == 'object':\n",
    "            # Try to convert to datetime\n",
    "            try:\n",
    "                pd.to_datetime(X_processed[col].head())\n",
    "                datetime_columns.append(col)\n",
    "                print(f\"Found datetime column: {col}\")\n",
    "            except:\n",
    "                print(f\"Column {col} is object type but not datetime\")\n",
    "    \n",
    "    # Process each datetime column\n",
    "    for col in datetime_columns:\n",
    "        print(f\"Processing datetime column: {col}\")\n",
    "        \n",
    "        # Convert to datetime\n",
    "        X_processed[col] = pd.to_datetime(X_processed[col])\n",
    "        \n",
    "        # Extract temporal features\n",
    "        X_processed[f'{col}_year'] = X_processed[col].dt.year\n",
    "        X_processed[f'{col}_month'] = X_processed[col].dt.month\n",
    "        X_processed[f'{col}_day'] = X_processed[col].dt.day\n",
    "        X_processed[f'{col}_hour'] = X_processed[col].dt.hour\n",
    "        X_processed[f'{col}_minute'] = X_processed[col].dt.minute\n",
    "        X_processed[f'{col}_dayofweek'] = X_processed[col].dt.dayofweek\n",
    "        X_processed[f'{col}_dayofyear'] = X_processed[col].dt.dayofyear\n",
    "        \n",
    "        # Cyclical encoding for periodic features\n",
    "        X_processed[f'{col}_hour_sin'] = np.sin(2 * np.pi * X_processed[f'{col}_hour'] / 24)\n",
    "        X_processed[f'{col}_hour_cos'] = np.cos(2 * np.pi * X_processed[f'{col}_hour'] / 24)\n",
    "        X_processed[f'{col}_month_sin'] = np.sin(2 * np.pi * X_processed[f'{col}_month'] / 12)\n",
    "        X_processed[f'{col}_month_cos'] = np.cos(2 * np.pi * X_processed[f'{col}_month'] / 12)\n",
    "        X_processed[f'{col}_dayofweek_sin'] = np.sin(2 * np.pi * X_processed[f'{col}_dayofweek'] / 7)\n",
    "        X_processed[f'{col}_dayofweek_cos'] = np.cos(2 * np.pi * X_processed[f'{col}_dayofweek'] / 7)\n",
    "        \n",
    "        # Remove original datetime column\n",
    "        X_processed = X_processed.drop(columns=[col])\n",
    "    \n",
    "    print(f\"Original features: {len(X.columns)}\")\n",
    "    print(f\"Processed features: {len(X_processed.columns)}\")\n",
    "    print(f\"New feature columns: {X_processed.columns.tolist()}\")\n",
    "    \n",
    "    return X_processed\n",
    "\n",
    "# Apply datetime preprocessing\n",
    "X_processed = preprocess_datetime_features(X, y)\n",
    "\n",
    "# Debug the processed data\n",
    "X_debug, y_debug = debug_and_preprocess_data(X_processed, y)\n",
    "\n",
    "# Then preprocess with the fixed class\n",
    "preprocessor = TimeSeriesPreprocessor(\n",
    "    sequence_length=24,\n",
    "    prediction_horizon=1,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "# Now preprocess with the debugged target\n",
    "X_train, X_test, y_train, y_test = preprocessor.fit_transform(X_debug, y_debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34496601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
